{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Chapter_9_DQN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cxbxmxcx/EatNoEat/blob/master/Chapter_9_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3H7uYopM09C",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP2RwX45M09D",
        "colab_type": "text"
      },
      "source": [
        "In 2015, Google DeepMind ([Link](https://deepmind.com/research/dqn/)) published a paper in Nature magazine that combines a deep convolution neural network with reinforcement learning for the first time in order to master a range of Atari 2600 games. They used only the raw pixels and score as the inputs. They were able to use the convolution layer to translate the pixels.  \n",
        "\n",
        "The very simple description is that they replaced the Q table in a Q-Learner with a neural network. This allowed them to take advantage of neural networks but still use reinforcement learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YN990GRM09E",
        "colab_type": "code",
        "outputId": "05094bdc-a2d3-4902-d137-0fe5bda6b66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Imports\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "#Create Gym\n",
        "from gym import wrappers\n",
        "envCartPole = gym.make('CartPole-v1')\n",
        "envCartPole.seed(50) #Set the seed to keep the environment consistent across runs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feK64CQaM09M",
        "colab_type": "text"
      },
      "source": [
        "**Experience Replay**  \n",
        "Definition: A mechanism inspired by biology that randomizes over the data removing the correlation in the observation sequence and smoothing over changes in the data distribution.  \n",
        "\n",
        "To perform an experience replay, the algorithm stores all of the agents experiences {$s_t,a_t,r_t,s_{t+1}$} at each time step in a data set. Normally in a q-learner, we would run the update rule on them. But, with experience replay we just store them.  \n",
        "\n",
        "Later during the training process these replays will be drawn uniformly from the memory queue and be ran through the update rule. There are 2 ways to handle this and I have coded both in the past. The first is to run them on every loop and the other is to run them after X amount of runs. In this code below, I run them each time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adn0QbIFM09N",
        "colab_type": "text"
      },
      "source": [
        "**Side Track: Vectorization**  \n",
        "I am going to take some time to talk about vectorization. If you are experienced with python you can skip this part. But, I went from running 100 episodes in mutliple minutes to being able to run 500 in less than 1.  \n",
        "\n",
        "The idea is that you execute the same task on ALL entries in an array at the same time.  \n",
        "\n",
        "Old way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntN2O4_kM09O",
        "colab_type": "code",
        "outputId": "bcd65093-1486-477f-a5f2-150195d5694f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tmp_array = []\n",
        "for i in range(100):\n",
        "    tmp_array.append(i)\n",
        "    \n",
        "#Add 10 to each element\n",
        "for i in range(100):\n",
        "    tmp_array[i] += 10\n",
        "print(tmp_array)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAthNxWWM09U",
        "colab_type": "text"
      },
      "source": [
        "Vectorized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoxENLSnM09U",
        "colab_type": "code",
        "outputId": "e2178c09-9f7d-4de4-9742-aa565b30f034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "tmp_array = []\n",
        "for i in range(100):\n",
        "    tmp_array.append(i)\n",
        "    \n",
        "#Add 10 to each element\n",
        "tmp_array = np.array(tmp_array) + 10\n",
        "print(tmp_array)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27\n",
            "  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45\n",
            "  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63\n",
            "  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 106 107 108 109]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usHDjRI1M09Z",
        "colab_type": "text"
      },
      "source": [
        "Most of these vectorizations will be calling a method versus just adding 10 but this is a simple solution. You will see a TREMENDOUS speed up avoiding the loops in python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_TNwRlBM09a",
        "colab_type": "text"
      },
      "source": [
        "**CartPole Example**  \n",
        "Again we will use the [CartPole](https://gym.openai.com/envs/CartPole-v1/) environment from OpenAI.  \n",
        "\n",
        "The actions are 0 to push the cart to the left and 1 to push the cart to the right.  \n",
        "\n",
        "The continuous state space is an X coordinate for location, the velocity of the cart, the angle of the pole, and the velocity at the tip of the pole. The X coordinate goes from -4.8 to +4.8, velocity is -Inf to +Inf, angle of the pole goes from -24 degrees to +24 degrees, tip velocity is -Inf to +Inf. With all of the possible combinations you can see why we can't create a Q table for each one.  \n",
        "\n",
        "To \"solve\" this puzzle you have to have an average reward of > 195 over 100 consecutive episodes. One thing to note, I am hard capping the rewards at 210 so this number can't average above that and it also could potentially drive the average down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID7OEx7sM09b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Global Variables\n",
        "EPISODES = 500\n",
        "TRAIN_END = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aByOFVVM09g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hyper Parameters\n",
        "def discount_rate(): #Gamma\n",
        "    return 0.95\n",
        "\n",
        "def learning_rate(): #Alpha\n",
        "    return 0.001\n",
        "\n",
        "def batch_size(): #Size of the batch used in the experience replay\n",
        "    return 24"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7kzPGJbM09l",
        "colab_type": "text"
      },
      "source": [
        "**Deep Q-Network Class**  \n",
        "The following class is the deep Q-network that is built using the neural network code from Keras.  \n",
        "**init**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This creates the class and sets the local parameters.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I use a *deque* for the local memory to hold the experiences and a keras model for the NN.  \n",
        "\n",
        "**build_model(self)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This builds the NN. I am using sequential model. Each of the layers are *Dense* despite the fact the document talks about using *Convolution*. But, they are only using that because they need to convert pixels and I already have numbers.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I am using an input layer(4), 24 neuron layer, 24 neuron layer, and an output layer(2).  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For calculating the loss I am using mean squared error.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For an optimizer I am using [Adam](https://arxiv.org/abs/1412.6980v8). It is a variant of gradient descent and you can read the technical document at the link. If you want a slightly lighter explaining you can check out [Machine Learning Mastery](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). You could also use SGD (Stochastic Gradient Descent) but Adam gives me better results and seems to be the standard in most examples.  \n",
        "\n",
        "**action(self,state)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This generates the action.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Explore: I am using the epsilon like previous lessons.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exploit: I use the NN to grab the 2 possible actions and then grab the argmax to find the better one  \n",
        "\n",
        "**test_action(self,state)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This generates the action when I am testing. I want to 100% exploit  \n",
        "\n",
        "**store(self, state, action, reward, nstate, done)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This places the observables in memory  \n",
        "\n",
        "**experience_replay(self, batch_size)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is where the training occurs. We grab the sample batches and then use the NN to predict the optimal action.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyRN6thxM09m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepQNetwork():\n",
        "    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay):\n",
        "        self.nS = states\n",
        "        self.nA = actions\n",
        "        self.memory = deque([], maxlen=2500)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        #Explore/Exploit\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = self.build_model()\n",
        "        self.loss = []\n",
        "        \n",
        "    def build_model(self):\n",
        "        model = keras.Sequential() #linear stack of layers https://keras.io/models/sequential/\n",
        "        model.add(keras.layers.Dense(24, input_dim=self.nS, activation='relu')) #[Input] -> Layer 1\n",
        "        #   Dense: Densely connected layer https://keras.io/layers/core/\n",
        "        #   24: Number of neurons\n",
        "        #   input_dim: Number of input variables\n",
        "        #   activation: Rectified Linear Unit (relu) ranges >= 0\n",
        "        model.add(keras.layers.Dense(24, activation='relu')) #Layer 2 -> 3\n",
        "        model.add(keras.layers.Dense(self.nA, activation='linear')) #Layer 3 -> [output]\n",
        "        #   Size has to match the output (different actions)\n",
        "        #   Linear activation on the last layer\n",
        "        model.compile(loss='mean_squared_error', #Loss function: Mean Squared Error\n",
        "                      optimizer=keras.optimizers.Adam(lr=self.alpha)) #Optimaizer: Adam (Feel free to check other options)\n",
        "        return model\n",
        "\n",
        "    def action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.nA) #Explore\n",
        "        action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def test_action(self, state): #Exploit\n",
        "        action_vals = self.model.predict(state)\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def store(self, state, action, reward, nstate, done):\n",
        "        #Store the experience in memory\n",
        "        self.memory.append( (state, action, reward, nstate, done) )\n",
        "\n",
        "    def experience_replay(self, batch_size):\n",
        "        #Execute the experience replay\n",
        "        minibatch = random.sample( self.memory, batch_size ) #Randomly sample from memory\n",
        "\n",
        "        #Convert to numpy for speed by vectorization\n",
        "        x = []\n",
        "        y = []\n",
        "        np_array = np.array(minibatch)\n",
        "        st = np.zeros((0,self.nS)) #States\n",
        "        nst = np.zeros( (0,self.nS) )#Next States\n",
        "        for i in range(len(np_array)): #Creating the state and next state np arrays\n",
        "            st = np.append( st, np_array[i,0], axis=0)\n",
        "            nst = np.append( nst, np_array[i,3], axis=0)\n",
        "        st_predict = self.model.predict(st) #Here is the speedup! I can predict on the ENTIRE batch\n",
        "        nst_predict = self.model.predict(nst)\n",
        "        index = 0\n",
        "        for state, action, reward, nstate, done in minibatch:\n",
        "            x.append(state)\n",
        "            #Predict from state\n",
        "            nst_action_predict_model = nst_predict[index]\n",
        "            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n",
        "                target = reward\n",
        "            else:   #Non terminal\n",
        "                target = reward + self.gamma * np.amax(nst_action_predict_model)\n",
        "            target_f = st_predict[index]\n",
        "            target_f[action] = target\n",
        "            y.append(target_f)\n",
        "            index += 1\n",
        "        #Reshape for Keras Fit\n",
        "        x_reshape = np.array(x).reshape(batch_size,self.nS)\n",
        "        y_reshape = np.array(y)\n",
        "        epoch_count = 1 #Epochs is the number or iterations\n",
        "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
        "        #Graph Losses\n",
        "        for i in range(epoch_count):\n",
        "            self.loss.append( hist.history['loss'][i] )\n",
        "        #Decay Epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0tnt0rsM09s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the agent\n",
        "nS = envCartPole.observation_space.shape[0] #This is only 4\n",
        "nA = envCartPole.action_space.n #Actions\n",
        "dqn = DeepQNetwork(nS, nA, learning_rate(), discount_rate(), 1, 0.001, 0.995 )\n",
        "\n",
        "batch_size = batch_size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t4Om9isM09w",
        "colab_type": "code",
        "outputId": "eb031385-acad-498c-b991-cf1221e3c89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Training\n",
        "rewards = [] #Store rewards for graphing\n",
        "epsilons = [] # Store the Explore/Exploit\n",
        "TEST_Episodes = 0\n",
        "for e in range(EPISODES):\n",
        "    state = envCartPole.reset()\n",
        "    state = np.reshape(state, [1, nS]) # Resize to store in memory to pass to .predict\n",
        "    tot_rewards = 0\n",
        "    for time in range(210): #200 is when you \"solve\" the game. This can continue forever as far as I know\n",
        "        action = dqn.action(state)\n",
        "        nstate, reward, done, _ = envCartPole.step(action)\n",
        "        nstate = np.reshape(nstate, [1, nS])\n",
        "        tot_rewards += reward\n",
        "        dqn.store(state, action, reward, nstate, done) # Resize to store in memory to pass to .predict\n",
        "        state = nstate\n",
        "        #done: CartPole fell. \n",
        "        #time == 209: CartPole stayed upright\n",
        "        if done or time == 209:\n",
        "            rewards.append(tot_rewards)\n",
        "            epsilons.append(dqn.epsilon)\n",
        "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
        "                  .format(e, EPISODES, tot_rewards, dqn.epsilon))\n",
        "            break\n",
        "        #Experience Replay\n",
        "        if len(dqn.memory) > batch_size:\n",
        "            dqn.experience_replay(batch_size)\n",
        "    #If our current NN passes we are done\n",
        "    #I am going to use the last 5 runs\n",
        "    if len(rewards) > 5 and np.average(rewards[-5:]) > 195:\n",
        "        #Set the rest of the EPISODES for testing\n",
        "        TEST_Episodes = EPISODES - e\n",
        "        TRAIN_END = e\n",
        "        break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0/500, score: 13.0, e: 1\n",
            "episode: 1/500, score: 10.0, e: 1\n",
            "episode: 2/500, score: 17.0, e: 0.9275689688183278\n",
            "episode: 3/500, score: 19.0, e: 0.8475428503023453\n",
            "episode: 4/500, score: 35.0, e: 0.7147372386831305\n",
            "episode: 5/500, score: 12.0, e: 0.6763948591909945\n",
            "episode: 6/500, score: 16.0, e: 0.6274028820538087\n",
            "episode: 7/500, score: 13.0, e: 0.5907768628656763\n",
            "episode: 8/500, score: 17.0, e: 0.5452463540625918\n",
            "episode: 9/500, score: 14.0, e: 0.510849320360386\n",
            "episode: 10/500, score: 12.0, e: 0.483444593917636\n",
            "episode: 11/500, score: 32.0, e: 0.41386834584198684\n",
            "episode: 12/500, score: 17.0, e: 0.3819719776053028\n",
            "episode: 13/500, score: 13.0, e: 0.3596735257153405\n",
            "episode: 14/500, score: 10.0, e: 0.3438081748424137\n",
            "episode: 15/500, score: 10.0, e: 0.32864265128599696\n",
            "episode: 16/500, score: 10.0, e: 0.3141460853680822\n",
            "episode: 17/500, score: 13.0, e: 0.29580711868545667\n",
            "episode: 18/500, score: 12.0, e: 0.2799384215094006\n",
            "episode: 19/500, score: 11.0, e: 0.2662522686041882\n",
            "episode: 20/500, score: 11.0, e: 0.2532352299289372\n",
            "episode: 21/500, score: 11.0, e: 0.2408545925762412\n",
            "episode: 22/500, score: 12.0, e: 0.22793384675362674\n",
            "episode: 23/500, score: 10.0, e: 0.2178795886667409\n",
            "episode: 24/500, score: 10.0, e: 0.20826882814336947\n",
            "episode: 25/500, score: 9.0, e: 0.2000824143909432\n",
            "episode: 26/500, score: 12.0, e: 0.18934890919900021\n",
            "episode: 27/500, score: 13.0, e: 0.17829525136613786\n",
            "episode: 28/500, score: 21.0, e: 0.16128775296900558\n",
            "episode: 29/500, score: 41.0, e: 0.13198501352907185\n",
            "episode: 30/500, score: 33.0, e: 0.11242507996344034\n",
            "episode: 31/500, score: 10.0, e: 0.10746596228306791\n",
            "episode: 32/500, score: 67.0, e: 0.07719590465791494\n",
            "episode: 33/500, score: 84.0, e: 0.05092252885731386\n",
            "episode: 34/500, score: 49.0, e: 0.040032972548509065\n",
            "episode: 35/500, score: 61.0, e: 0.029634846598205186\n",
            "episode: 36/500, score: 193.0, e: 0.011319680961146208\n",
            "episode: 37/500, score: 93.0, e: 0.007137688903470108\n",
            "episode: 38/500, score: 185.0, e: 0.002837948254359161\n",
            "episode: 39/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 40/500, score: 197.0, e: 0.0009954703940636294\n",
            "episode: 41/500, score: 147.0, e: 0.0009954703940636294\n",
            "episode: 42/500, score: 102.0, e: 0.0009954703940636294\n",
            "episode: 43/500, score: 102.0, e: 0.0009954703940636294\n",
            "episode: 44/500, score: 72.0, e: 0.0009954703940636294\n",
            "episode: 45/500, score: 25.0, e: 0.0009954703940636294\n",
            "episode: 46/500, score: 56.0, e: 0.0009954703940636294\n",
            "episode: 47/500, score: 62.0, e: 0.0009954703940636294\n",
            "episode: 48/500, score: 59.0, e: 0.0009954703940636294\n",
            "episode: 49/500, score: 64.0, e: 0.0009954703940636294\n",
            "episode: 50/500, score: 73.0, e: 0.0009954703940636294\n",
            "episode: 51/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 52/500, score: 111.0, e: 0.0009954703940636294\n",
            "episode: 53/500, score: 69.0, e: 0.0009954703940636294\n",
            "episode: 54/500, score: 69.0, e: 0.0009954703940636294\n",
            "episode: 55/500, score: 118.0, e: 0.0009954703940636294\n",
            "episode: 56/500, score: 182.0, e: 0.0009954703940636294\n",
            "episode: 57/500, score: 209.0, e: 0.0009954703940636294\n",
            "episode: 58/500, score: 121.0, e: 0.0009954703940636294\n",
            "episode: 59/500, score: 79.0, e: 0.0009954703940636294\n",
            "episode: 60/500, score: 96.0, e: 0.0009954703940636294\n",
            "episode: 61/500, score: 120.0, e: 0.0009954703940636294\n",
            "episode: 62/500, score: 167.0, e: 0.0009954703940636294\n",
            "episode: 63/500, score: 71.0, e: 0.0009954703940636294\n",
            "episode: 64/500, score: 150.0, e: 0.0009954703940636294\n",
            "episode: 65/500, score: 131.0, e: 0.0009954703940636294\n",
            "episode: 66/500, score: 114.0, e: 0.0009954703940636294\n",
            "episode: 67/500, score: 98.0, e: 0.0009954703940636294\n",
            "episode: 68/500, score: 70.0, e: 0.0009954703940636294\n",
            "episode: 69/500, score: 82.0, e: 0.0009954703940636294\n",
            "episode: 70/500, score: 144.0, e: 0.0009954703940636294\n",
            "episode: 71/500, score: 68.0, e: 0.0009954703940636294\n",
            "episode: 72/500, score: 35.0, e: 0.0009954703940636294\n",
            "episode: 73/500, score: 33.0, e: 0.0009954703940636294\n",
            "episode: 74/500, score: 44.0, e: 0.0009954703940636294\n",
            "episode: 75/500, score: 95.0, e: 0.0009954703940636294\n",
            "episode: 76/500, score: 106.0, e: 0.0009954703940636294\n",
            "episode: 77/500, score: 124.0, e: 0.0009954703940636294\n",
            "episode: 78/500, score: 122.0, e: 0.0009954703940636294\n",
            "episode: 79/500, score: 174.0, e: 0.0009954703940636294\n",
            "episode: 80/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 81/500, score: 77.0, e: 0.0009954703940636294\n",
            "episode: 82/500, score: 55.0, e: 0.0009954703940636294\n",
            "episode: 83/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 84/500, score: 67.0, e: 0.0009954703940636294\n",
            "episode: 85/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 86/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 87/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 88/500, score: 210.0, e: 0.0009954703940636294\n",
            "episode: 89/500, score: 210.0, e: 0.0009954703940636294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CKciTSeM091",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c7e55c5-e742-462c-8e67-b50003bf163a"
      },
      "source": [
        "#Test the agent that was trained\n",
        "#   In this section we ALWAYS use exploit don't train any more\n",
        "for e_test in range(TEST_Episodes):\n",
        "    state = envCartPole.reset()\n",
        "    state = np.reshape(state, [1, nS])\n",
        "    tot_rewards = 0\n",
        "    for t_test in range(210):\n",
        "        action = dqn.test_action(state)\n",
        "        nstate, reward, done, _ = envCartPole.step(action)\n",
        "        nstate = np.reshape( nstate, [1, nS])\n",
        "        tot_rewards += reward\n",
        "        #DON'T STORE ANYTHING DURING TESTING\n",
        "        state = nstate\n",
        "        #done: CartPole fell. \n",
        "        #t_test == 209: CartPole stayed upright\n",
        "        if done or t_test == 209: \n",
        "            rewards.append(tot_rewards)\n",
        "            epsilons.append(0) #We are doing full exploit\n",
        "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
        "                  .format(e_test, TEST_Episodes, tot_rewards, 0))\n",
        "            break;"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0/411, score: 210.0, e: 0\n",
            "episode: 1/411, score: 210.0, e: 0\n",
            "episode: 2/411, score: 210.0, e: 0\n",
            "episode: 3/411, score: 210.0, e: 0\n",
            "episode: 4/411, score: 210.0, e: 0\n",
            "episode: 5/411, score: 210.0, e: 0\n",
            "episode: 6/411, score: 210.0, e: 0\n",
            "episode: 7/411, score: 193.0, e: 0\n",
            "episode: 8/411, score: 210.0, e: 0\n",
            "episode: 9/411, score: 210.0, e: 0\n",
            "episode: 10/411, score: 210.0, e: 0\n",
            "episode: 11/411, score: 210.0, e: 0\n",
            "episode: 12/411, score: 210.0, e: 0\n",
            "episode: 13/411, score: 210.0, e: 0\n",
            "episode: 14/411, score: 210.0, e: 0\n",
            "episode: 15/411, score: 210.0, e: 0\n",
            "episode: 16/411, score: 210.0, e: 0\n",
            "episode: 17/411, score: 210.0, e: 0\n",
            "episode: 18/411, score: 210.0, e: 0\n",
            "episode: 19/411, score: 210.0, e: 0\n",
            "episode: 20/411, score: 210.0, e: 0\n",
            "episode: 21/411, score: 210.0, e: 0\n",
            "episode: 22/411, score: 210.0, e: 0\n",
            "episode: 23/411, score: 210.0, e: 0\n",
            "episode: 24/411, score: 210.0, e: 0\n",
            "episode: 25/411, score: 210.0, e: 0\n",
            "episode: 26/411, score: 210.0, e: 0\n",
            "episode: 27/411, score: 210.0, e: 0\n",
            "episode: 28/411, score: 210.0, e: 0\n",
            "episode: 29/411, score: 210.0, e: 0\n",
            "episode: 30/411, score: 210.0, e: 0\n",
            "episode: 31/411, score: 210.0, e: 0\n",
            "episode: 32/411, score: 192.0, e: 0\n",
            "episode: 33/411, score: 210.0, e: 0\n",
            "episode: 34/411, score: 210.0, e: 0\n",
            "episode: 35/411, score: 210.0, e: 0\n",
            "episode: 36/411, score: 210.0, e: 0\n",
            "episode: 37/411, score: 210.0, e: 0\n",
            "episode: 38/411, score: 210.0, e: 0\n",
            "episode: 39/411, score: 210.0, e: 0\n",
            "episode: 40/411, score: 210.0, e: 0\n",
            "episode: 41/411, score: 210.0, e: 0\n",
            "episode: 42/411, score: 210.0, e: 0\n",
            "episode: 43/411, score: 210.0, e: 0\n",
            "episode: 44/411, score: 210.0, e: 0\n",
            "episode: 45/411, score: 210.0, e: 0\n",
            "episode: 46/411, score: 210.0, e: 0\n",
            "episode: 47/411, score: 210.0, e: 0\n",
            "episode: 48/411, score: 174.0, e: 0\n",
            "episode: 49/411, score: 210.0, e: 0\n",
            "episode: 50/411, score: 210.0, e: 0\n",
            "episode: 51/411, score: 210.0, e: 0\n",
            "episode: 52/411, score: 210.0, e: 0\n",
            "episode: 53/411, score: 210.0, e: 0\n",
            "episode: 54/411, score: 210.0, e: 0\n",
            "episode: 55/411, score: 210.0, e: 0\n",
            "episode: 56/411, score: 210.0, e: 0\n",
            "episode: 57/411, score: 210.0, e: 0\n",
            "episode: 58/411, score: 210.0, e: 0\n",
            "episode: 59/411, score: 210.0, e: 0\n",
            "episode: 60/411, score: 210.0, e: 0\n",
            "episode: 61/411, score: 210.0, e: 0\n",
            "episode: 62/411, score: 210.0, e: 0\n",
            "episode: 63/411, score: 210.0, e: 0\n",
            "episode: 64/411, score: 210.0, e: 0\n",
            "episode: 65/411, score: 210.0, e: 0\n",
            "episode: 66/411, score: 210.0, e: 0\n",
            "episode: 67/411, score: 210.0, e: 0\n",
            "episode: 68/411, score: 210.0, e: 0\n",
            "episode: 69/411, score: 210.0, e: 0\n",
            "episode: 70/411, score: 210.0, e: 0\n",
            "episode: 71/411, score: 210.0, e: 0\n",
            "episode: 72/411, score: 210.0, e: 0\n",
            "episode: 73/411, score: 210.0, e: 0\n",
            "episode: 74/411, score: 210.0, e: 0\n",
            "episode: 75/411, score: 210.0, e: 0\n",
            "episode: 76/411, score: 210.0, e: 0\n",
            "episode: 77/411, score: 210.0, e: 0\n",
            "episode: 78/411, score: 210.0, e: 0\n",
            "episode: 79/411, score: 210.0, e: 0\n",
            "episode: 80/411, score: 210.0, e: 0\n",
            "episode: 81/411, score: 210.0, e: 0\n",
            "episode: 82/411, score: 210.0, e: 0\n",
            "episode: 83/411, score: 210.0, e: 0\n",
            "episode: 84/411, score: 210.0, e: 0\n",
            "episode: 85/411, score: 210.0, e: 0\n",
            "episode: 86/411, score: 210.0, e: 0\n",
            "episode: 87/411, score: 210.0, e: 0\n",
            "episode: 88/411, score: 210.0, e: 0\n",
            "episode: 89/411, score: 210.0, e: 0\n",
            "episode: 90/411, score: 210.0, e: 0\n",
            "episode: 91/411, score: 210.0, e: 0\n",
            "episode: 92/411, score: 210.0, e: 0\n",
            "episode: 93/411, score: 178.0, e: 0\n",
            "episode: 94/411, score: 210.0, e: 0\n",
            "episode: 95/411, score: 210.0, e: 0\n",
            "episode: 96/411, score: 210.0, e: 0\n",
            "episode: 97/411, score: 210.0, e: 0\n",
            "episode: 98/411, score: 210.0, e: 0\n",
            "episode: 99/411, score: 210.0, e: 0\n",
            "episode: 100/411, score: 210.0, e: 0\n",
            "episode: 101/411, score: 210.0, e: 0\n",
            "episode: 102/411, score: 210.0, e: 0\n",
            "episode: 103/411, score: 195.0, e: 0\n",
            "episode: 104/411, score: 210.0, e: 0\n",
            "episode: 105/411, score: 210.0, e: 0\n",
            "episode: 106/411, score: 210.0, e: 0\n",
            "episode: 107/411, score: 210.0, e: 0\n",
            "episode: 108/411, score: 210.0, e: 0\n",
            "episode: 109/411, score: 210.0, e: 0\n",
            "episode: 110/411, score: 210.0, e: 0\n",
            "episode: 111/411, score: 210.0, e: 0\n",
            "episode: 112/411, score: 210.0, e: 0\n",
            "episode: 113/411, score: 210.0, e: 0\n",
            "episode: 114/411, score: 210.0, e: 0\n",
            "episode: 115/411, score: 210.0, e: 0\n",
            "episode: 116/411, score: 192.0, e: 0\n",
            "episode: 117/411, score: 210.0, e: 0\n",
            "episode: 118/411, score: 210.0, e: 0\n",
            "episode: 119/411, score: 210.0, e: 0\n",
            "episode: 120/411, score: 210.0, e: 0\n",
            "episode: 121/411, score: 210.0, e: 0\n",
            "episode: 122/411, score: 210.0, e: 0\n",
            "episode: 123/411, score: 210.0, e: 0\n",
            "episode: 124/411, score: 210.0, e: 0\n",
            "episode: 125/411, score: 210.0, e: 0\n",
            "episode: 126/411, score: 210.0, e: 0\n",
            "episode: 127/411, score: 210.0, e: 0\n",
            "episode: 128/411, score: 210.0, e: 0\n",
            "episode: 129/411, score: 210.0, e: 0\n",
            "episode: 130/411, score: 210.0, e: 0\n",
            "episode: 131/411, score: 210.0, e: 0\n",
            "episode: 132/411, score: 210.0, e: 0\n",
            "episode: 133/411, score: 210.0, e: 0\n",
            "episode: 134/411, score: 210.0, e: 0\n",
            "episode: 135/411, score: 210.0, e: 0\n",
            "episode: 136/411, score: 210.0, e: 0\n",
            "episode: 137/411, score: 210.0, e: 0\n",
            "episode: 138/411, score: 210.0, e: 0\n",
            "episode: 139/411, score: 210.0, e: 0\n",
            "episode: 140/411, score: 210.0, e: 0\n",
            "episode: 141/411, score: 210.0, e: 0\n",
            "episode: 142/411, score: 210.0, e: 0\n",
            "episode: 143/411, score: 210.0, e: 0\n",
            "episode: 144/411, score: 210.0, e: 0\n",
            "episode: 145/411, score: 210.0, e: 0\n",
            "episode: 146/411, score: 210.0, e: 0\n",
            "episode: 147/411, score: 210.0, e: 0\n",
            "episode: 148/411, score: 210.0, e: 0\n",
            "episode: 149/411, score: 210.0, e: 0\n",
            "episode: 150/411, score: 210.0, e: 0\n",
            "episode: 151/411, score: 210.0, e: 0\n",
            "episode: 152/411, score: 210.0, e: 0\n",
            "episode: 153/411, score: 210.0, e: 0\n",
            "episode: 154/411, score: 210.0, e: 0\n",
            "episode: 155/411, score: 210.0, e: 0\n",
            "episode: 156/411, score: 210.0, e: 0\n",
            "episode: 157/411, score: 210.0, e: 0\n",
            "episode: 158/411, score: 210.0, e: 0\n",
            "episode: 159/411, score: 210.0, e: 0\n",
            "episode: 160/411, score: 210.0, e: 0\n",
            "episode: 161/411, score: 210.0, e: 0\n",
            "episode: 162/411, score: 210.0, e: 0\n",
            "episode: 163/411, score: 210.0, e: 0\n",
            "episode: 164/411, score: 210.0, e: 0\n",
            "episode: 165/411, score: 210.0, e: 0\n",
            "episode: 166/411, score: 210.0, e: 0\n",
            "episode: 167/411, score: 210.0, e: 0\n",
            "episode: 168/411, score: 210.0, e: 0\n",
            "episode: 169/411, score: 210.0, e: 0\n",
            "episode: 170/411, score: 210.0, e: 0\n",
            "episode: 171/411, score: 210.0, e: 0\n",
            "episode: 172/411, score: 210.0, e: 0\n",
            "episode: 173/411, score: 210.0, e: 0\n",
            "episode: 174/411, score: 210.0, e: 0\n",
            "episode: 175/411, score: 210.0, e: 0\n",
            "episode: 176/411, score: 210.0, e: 0\n",
            "episode: 177/411, score: 210.0, e: 0\n",
            "episode: 178/411, score: 210.0, e: 0\n",
            "episode: 179/411, score: 210.0, e: 0\n",
            "episode: 180/411, score: 176.0, e: 0\n",
            "episode: 181/411, score: 210.0, e: 0\n",
            "episode: 182/411, score: 210.0, e: 0\n",
            "episode: 183/411, score: 210.0, e: 0\n",
            "episode: 184/411, score: 210.0, e: 0\n",
            "episode: 185/411, score: 210.0, e: 0\n",
            "episode: 186/411, score: 210.0, e: 0\n",
            "episode: 187/411, score: 210.0, e: 0\n",
            "episode: 188/411, score: 210.0, e: 0\n",
            "episode: 189/411, score: 210.0, e: 0\n",
            "episode: 190/411, score: 210.0, e: 0\n",
            "episode: 191/411, score: 210.0, e: 0\n",
            "episode: 192/411, score: 210.0, e: 0\n",
            "episode: 193/411, score: 210.0, e: 0\n",
            "episode: 194/411, score: 210.0, e: 0\n",
            "episode: 195/411, score: 210.0, e: 0\n",
            "episode: 196/411, score: 210.0, e: 0\n",
            "episode: 197/411, score: 194.0, e: 0\n",
            "episode: 198/411, score: 210.0, e: 0\n",
            "episode: 199/411, score: 210.0, e: 0\n",
            "episode: 200/411, score: 210.0, e: 0\n",
            "episode: 201/411, score: 210.0, e: 0\n",
            "episode: 202/411, score: 210.0, e: 0\n",
            "episode: 203/411, score: 210.0, e: 0\n",
            "episode: 204/411, score: 210.0, e: 0\n",
            "episode: 205/411, score: 210.0, e: 0\n",
            "episode: 206/411, score: 196.0, e: 0\n",
            "episode: 207/411, score: 210.0, e: 0\n",
            "episode: 208/411, score: 210.0, e: 0\n",
            "episode: 209/411, score: 210.0, e: 0\n",
            "episode: 210/411, score: 210.0, e: 0\n",
            "episode: 211/411, score: 210.0, e: 0\n",
            "episode: 212/411, score: 210.0, e: 0\n",
            "episode: 213/411, score: 210.0, e: 0\n",
            "episode: 214/411, score: 210.0, e: 0\n",
            "episode: 215/411, score: 210.0, e: 0\n",
            "episode: 216/411, score: 210.0, e: 0\n",
            "episode: 217/411, score: 210.0, e: 0\n",
            "episode: 218/411, score: 210.0, e: 0\n",
            "episode: 219/411, score: 210.0, e: 0\n",
            "episode: 220/411, score: 210.0, e: 0\n",
            "episode: 221/411, score: 210.0, e: 0\n",
            "episode: 222/411, score: 210.0, e: 0\n",
            "episode: 223/411, score: 210.0, e: 0\n",
            "episode: 224/411, score: 210.0, e: 0\n",
            "episode: 225/411, score: 210.0, e: 0\n",
            "episode: 226/411, score: 210.0, e: 0\n",
            "episode: 227/411, score: 210.0, e: 0\n",
            "episode: 228/411, score: 210.0, e: 0\n",
            "episode: 229/411, score: 210.0, e: 0\n",
            "episode: 230/411, score: 210.0, e: 0\n",
            "episode: 231/411, score: 210.0, e: 0\n",
            "episode: 232/411, score: 210.0, e: 0\n",
            "episode: 233/411, score: 210.0, e: 0\n",
            "episode: 234/411, score: 210.0, e: 0\n",
            "episode: 235/411, score: 210.0, e: 0\n",
            "episode: 236/411, score: 210.0, e: 0\n",
            "episode: 237/411, score: 210.0, e: 0\n",
            "episode: 238/411, score: 210.0, e: 0\n",
            "episode: 239/411, score: 210.0, e: 0\n",
            "episode: 240/411, score: 210.0, e: 0\n",
            "episode: 241/411, score: 210.0, e: 0\n",
            "episode: 242/411, score: 210.0, e: 0\n",
            "episode: 243/411, score: 210.0, e: 0\n",
            "episode: 244/411, score: 210.0, e: 0\n",
            "episode: 245/411, score: 210.0, e: 0\n",
            "episode: 246/411, score: 208.0, e: 0\n",
            "episode: 247/411, score: 210.0, e: 0\n",
            "episode: 248/411, score: 210.0, e: 0\n",
            "episode: 249/411, score: 210.0, e: 0\n",
            "episode: 250/411, score: 210.0, e: 0\n",
            "episode: 251/411, score: 210.0, e: 0\n",
            "episode: 252/411, score: 210.0, e: 0\n",
            "episode: 253/411, score: 210.0, e: 0\n",
            "episode: 254/411, score: 210.0, e: 0\n",
            "episode: 255/411, score: 210.0, e: 0\n",
            "episode: 256/411, score: 210.0, e: 0\n",
            "episode: 257/411, score: 210.0, e: 0\n",
            "episode: 258/411, score: 210.0, e: 0\n",
            "episode: 259/411, score: 210.0, e: 0\n",
            "episode: 260/411, score: 210.0, e: 0\n",
            "episode: 261/411, score: 210.0, e: 0\n",
            "episode: 262/411, score: 210.0, e: 0\n",
            "episode: 263/411, score: 210.0, e: 0\n",
            "episode: 264/411, score: 210.0, e: 0\n",
            "episode: 265/411, score: 210.0, e: 0\n",
            "episode: 266/411, score: 210.0, e: 0\n",
            "episode: 267/411, score: 210.0, e: 0\n",
            "episode: 268/411, score: 210.0, e: 0\n",
            "episode: 269/411, score: 210.0, e: 0\n",
            "episode: 270/411, score: 210.0, e: 0\n",
            "episode: 271/411, score: 210.0, e: 0\n",
            "episode: 272/411, score: 210.0, e: 0\n",
            "episode: 273/411, score: 210.0, e: 0\n",
            "episode: 274/411, score: 210.0, e: 0\n",
            "episode: 275/411, score: 210.0, e: 0\n",
            "episode: 276/411, score: 210.0, e: 0\n",
            "episode: 277/411, score: 210.0, e: 0\n",
            "episode: 278/411, score: 210.0, e: 0\n",
            "episode: 279/411, score: 210.0, e: 0\n",
            "episode: 280/411, score: 210.0, e: 0\n",
            "episode: 281/411, score: 210.0, e: 0\n",
            "episode: 282/411, score: 210.0, e: 0\n",
            "episode: 283/411, score: 210.0, e: 0\n",
            "episode: 284/411, score: 210.0, e: 0\n",
            "episode: 285/411, score: 210.0, e: 0\n",
            "episode: 286/411, score: 210.0, e: 0\n",
            "episode: 287/411, score: 210.0, e: 0\n",
            "episode: 288/411, score: 210.0, e: 0\n",
            "episode: 289/411, score: 210.0, e: 0\n",
            "episode: 290/411, score: 210.0, e: 0\n",
            "episode: 291/411, score: 210.0, e: 0\n",
            "episode: 292/411, score: 210.0, e: 0\n",
            "episode: 293/411, score: 210.0, e: 0\n",
            "episode: 294/411, score: 210.0, e: 0\n",
            "episode: 295/411, score: 210.0, e: 0\n",
            "episode: 296/411, score: 210.0, e: 0\n",
            "episode: 297/411, score: 210.0, e: 0\n",
            "episode: 298/411, score: 210.0, e: 0\n",
            "episode: 299/411, score: 210.0, e: 0\n",
            "episode: 300/411, score: 210.0, e: 0\n",
            "episode: 301/411, score: 210.0, e: 0\n",
            "episode: 302/411, score: 210.0, e: 0\n",
            "episode: 303/411, score: 210.0, e: 0\n",
            "episode: 304/411, score: 210.0, e: 0\n",
            "episode: 305/411, score: 210.0, e: 0\n",
            "episode: 306/411, score: 210.0, e: 0\n",
            "episode: 307/411, score: 210.0, e: 0\n",
            "episode: 308/411, score: 210.0, e: 0\n",
            "episode: 309/411, score: 210.0, e: 0\n",
            "episode: 310/411, score: 210.0, e: 0\n",
            "episode: 311/411, score: 210.0, e: 0\n",
            "episode: 312/411, score: 210.0, e: 0\n",
            "episode: 313/411, score: 210.0, e: 0\n",
            "episode: 314/411, score: 210.0, e: 0\n",
            "episode: 315/411, score: 176.0, e: 0\n",
            "episode: 316/411, score: 210.0, e: 0\n",
            "episode: 317/411, score: 210.0, e: 0\n",
            "episode: 318/411, score: 210.0, e: 0\n",
            "episode: 319/411, score: 210.0, e: 0\n",
            "episode: 320/411, score: 210.0, e: 0\n",
            "episode: 321/411, score: 210.0, e: 0\n",
            "episode: 322/411, score: 210.0, e: 0\n",
            "episode: 323/411, score: 210.0, e: 0\n",
            "episode: 324/411, score: 210.0, e: 0\n",
            "episode: 325/411, score: 210.0, e: 0\n",
            "episode: 326/411, score: 210.0, e: 0\n",
            "episode: 327/411, score: 210.0, e: 0\n",
            "episode: 328/411, score: 210.0, e: 0\n",
            "episode: 329/411, score: 210.0, e: 0\n",
            "episode: 330/411, score: 210.0, e: 0\n",
            "episode: 331/411, score: 210.0, e: 0\n",
            "episode: 332/411, score: 210.0, e: 0\n",
            "episode: 333/411, score: 210.0, e: 0\n",
            "episode: 334/411, score: 210.0, e: 0\n",
            "episode: 335/411, score: 210.0, e: 0\n",
            "episode: 336/411, score: 210.0, e: 0\n",
            "episode: 337/411, score: 210.0, e: 0\n",
            "episode: 338/411, score: 210.0, e: 0\n",
            "episode: 339/411, score: 210.0, e: 0\n",
            "episode: 340/411, score: 210.0, e: 0\n",
            "episode: 341/411, score: 210.0, e: 0\n",
            "episode: 342/411, score: 210.0, e: 0\n",
            "episode: 343/411, score: 189.0, e: 0\n",
            "episode: 344/411, score: 210.0, e: 0\n",
            "episode: 345/411, score: 210.0, e: 0\n",
            "episode: 346/411, score: 210.0, e: 0\n",
            "episode: 347/411, score: 210.0, e: 0\n",
            "episode: 348/411, score: 210.0, e: 0\n",
            "episode: 349/411, score: 210.0, e: 0\n",
            "episode: 350/411, score: 210.0, e: 0\n",
            "episode: 351/411, score: 210.0, e: 0\n",
            "episode: 352/411, score: 210.0, e: 0\n",
            "episode: 353/411, score: 210.0, e: 0\n",
            "episode: 354/411, score: 210.0, e: 0\n",
            "episode: 355/411, score: 210.0, e: 0\n",
            "episode: 356/411, score: 210.0, e: 0\n",
            "episode: 357/411, score: 210.0, e: 0\n",
            "episode: 358/411, score: 210.0, e: 0\n",
            "episode: 359/411, score: 210.0, e: 0\n",
            "episode: 360/411, score: 210.0, e: 0\n",
            "episode: 361/411, score: 210.0, e: 0\n",
            "episode: 362/411, score: 210.0, e: 0\n",
            "episode: 363/411, score: 210.0, e: 0\n",
            "episode: 364/411, score: 210.0, e: 0\n",
            "episode: 365/411, score: 210.0, e: 0\n",
            "episode: 366/411, score: 210.0, e: 0\n",
            "episode: 367/411, score: 210.0, e: 0\n",
            "episode: 368/411, score: 210.0, e: 0\n",
            "episode: 369/411, score: 210.0, e: 0\n",
            "episode: 370/411, score: 210.0, e: 0\n",
            "episode: 371/411, score: 210.0, e: 0\n",
            "episode: 372/411, score: 210.0, e: 0\n",
            "episode: 373/411, score: 210.0, e: 0\n",
            "episode: 374/411, score: 210.0, e: 0\n",
            "episode: 375/411, score: 210.0, e: 0\n",
            "episode: 376/411, score: 210.0, e: 0\n",
            "episode: 377/411, score: 187.0, e: 0\n",
            "episode: 378/411, score: 210.0, e: 0\n",
            "episode: 379/411, score: 210.0, e: 0\n",
            "episode: 380/411, score: 210.0, e: 0\n",
            "episode: 381/411, score: 210.0, e: 0\n",
            "episode: 382/411, score: 210.0, e: 0\n",
            "episode: 383/411, score: 210.0, e: 0\n",
            "episode: 384/411, score: 210.0, e: 0\n",
            "episode: 385/411, score: 210.0, e: 0\n",
            "episode: 386/411, score: 210.0, e: 0\n",
            "episode: 387/411, score: 210.0, e: 0\n",
            "episode: 388/411, score: 210.0, e: 0\n",
            "episode: 389/411, score: 210.0, e: 0\n",
            "episode: 390/411, score: 210.0, e: 0\n",
            "episode: 391/411, score: 201.0, e: 0\n",
            "episode: 392/411, score: 210.0, e: 0\n",
            "episode: 393/411, score: 210.0, e: 0\n",
            "episode: 394/411, score: 210.0, e: 0\n",
            "episode: 395/411, score: 210.0, e: 0\n",
            "episode: 396/411, score: 210.0, e: 0\n",
            "episode: 397/411, score: 210.0, e: 0\n",
            "episode: 398/411, score: 210.0, e: 0\n",
            "episode: 399/411, score: 210.0, e: 0\n",
            "episode: 400/411, score: 210.0, e: 0\n",
            "episode: 401/411, score: 210.0, e: 0\n",
            "episode: 402/411, score: 210.0, e: 0\n",
            "episode: 403/411, score: 210.0, e: 0\n",
            "episode: 404/411, score: 210.0, e: 0\n",
            "episode: 405/411, score: 184.0, e: 0\n",
            "episode: 406/411, score: 210.0, e: 0\n",
            "episode: 407/411, score: 210.0, e: 0\n",
            "episode: 408/411, score: 210.0, e: 0\n",
            "episode: 409/411, score: 210.0, e: 0\n",
            "episode: 410/411, score: 210.0, e: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfynoKFSuyp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d169e37-7765-4313-d55a-9108fac1b416"
      },
      "source": [
        "print(nS, nA)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT_YwvRFM095",
        "colab_type": "text"
      },
      "source": [
        "**Results**  \n",
        "Here is a graph of the results. If everything was done correctly you should see the rewards over the red line.  \n",
        "\n",
        "Black: This is the 100 episode rolling average  \n",
        "Red: This is the \"solved\" line at 195  \n",
        "Blue: This is the reward for each episode  \n",
        "Green: This is the value of epsilon scaled by 200  \n",
        "Yellow: This is where the tests started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_3uEfizM096",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "73dd8089-4014-41f8-966a-e6c8700815f1"
      },
      "source": [
        "rolling_average = np.convolve(rewards, np.ones(100)/100)\n",
        "\n",
        "plt.plot(rewards)\n",
        "plt.plot(rolling_average, color='black')\n",
        "plt.axhline(y=195, color='r', linestyle='-') #Solved Line\n",
        "#Scale Epsilon (0.001 - 1.0) to match reward (0 - 200) range\n",
        "eps_graph = [200*x for x in epsilons]\n",
        "plt.plot(eps_graph, color='g', linestyle='-')\n",
        "#Plot the line where TESTING begins\n",
        "plt.axvline(x=TRAIN_END, color='y', linestyle='-')\n",
        "plt.xlim( (0,EPISODES) )\n",
        "plt.ylim( (0,220) )\n",
        "plt.show()\n",
        "\n",
        "\n",
        "envCartPole.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdU00j-kM09_",
        "colab_type": "text"
      },
      "source": [
        "**Changes**  \n",
        "*hyper parameters*: You can alter alpha, gamma, batch size, and episode length to see what differences the algorithm returns.  \n",
        "*Training End*: You can also change the line where I only check the last 5 runs before switching to testing mode (if len(rewards) > 5 and np.average(rewards[-5:]) > 195:) as that doesn't prove it was solved. The reason I did this was because I wanted to limit the amount of runs I made.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67-FqFtPM0-A",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion**  \n",
        "This is a Deep Q-Network implementation. There are some changes you can make here and there but it follows the paper. Hopefully, you were able to understand the code as well as make your own version to compare with this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "FkdSAzY6M0-B",
        "colab_type": "text"
      },
      "source": [
        "**Reference**  \n",
        "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). *Human-level control through deep reinforcement learning*. Nature, 518(7540), 529"
      ]
    }
  ]
}